# -*- coding: utf-8 -*-
# """AI Powered MultiLanguage Translater.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1hGV8zCejp3sdOfujE_n1Vfm4P8NQEMVH
# """
# üåê AI-Powered Multilingual Translator Using M2M100 and Gradio
# In todays interconnected world, breaking language barriers is crucial for global communication across education, business, travel, and social platforms. This project addresses that need by building an AI-powered multilingual translator leveraging advanced transformer-based neural networks.
# At its core is Facebook M2M100 model ‚Äî one of the first many-to-many multilingual translation models. Unlike traditional systems that rely on English as an intermediary or require separate models for each language pair, M2M100 enables direct translation between 100+ languages within a single, unified model.
# The system features a Gradio-based interactive web interface, allowing users to easily input text, choose source and target languages, and receive real-time translation results ‚Äî all accessible through a simple link from Google Colab or Jupyter Notebook, with no setup required.

# üîë Key Features:
# üåç Supports 100+ languages including English, Hindi, French, German, Spanish, Chinese, Japanese, and Korean
# ‚öôÔ∏è Powered by Facebook M2M100 ‚Äî a state-of-the-art transformer model
# ‚òÅÔ∏è Runs serverless in Google Colab ‚Äî no need for dedicated hardware or backend deployment
# üñ•Ô∏è Interactive Gradio UI ‚Äî simple, shareable, and user-friendly
# üöÄ Real-time translations with seamless language switching

#Install dependencies
!pip install transformers sentencepiece gradio

import os
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import gradio as gr

os.environ.pop("HUGGINGFACE_TOKEN", None)

#Load tokenizer and model
model_name = "facebook/m2m100_418M"

tokenizer = M2M100Tokenizer.from_pretrained(model_name)
model = M2M100ForConditionalGeneration.from_pretrained(model_name)

print("Model and tokenizer loaded!")

def translate(text, src_lang, tgt_lang):
    tokenizer.src_lang = src_lang
    encoded = tokenizer(text, return_tensors="pt")
    generated_tokens = model.generate(
        **encoded,
        forced_bos_token_id=tokenizer.get_lang_id(tgt_lang)
    )
    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated[0]

src_langs = [
    "en", "hi", "fr", "de", "es", "zh", "ja", "ko"
]

with gr.Blocks() as demo:
    gr.Markdown("#Multi-Language Translator (M2M100)")
    with gr.Row():
        input_text = gr.Textbox(label="Enter text to translate")
    with gr.Row():
        src = gr.Dropdown(choices=src_langs, value="en", label="Source Language")
        tgt = gr.Dropdown(choices=src_langs, value="hi", label="Target Language")
    with gr.Row():
        translate_btn = gr.Button("Translate")
    output = gr.Textbox(label="Translation Output")

    translate_btn.click(translate, inputs=[input_text, src, tgt], outputs=output)

#Launch it ‚Äî works in Colab!
demo.launch(share=True)
